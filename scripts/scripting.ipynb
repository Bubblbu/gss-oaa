{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Access Award\n",
    "> Scripts to run and adjucate the GSS Open Access Award at SFU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"../oaa-2018/data/\")\n",
    "applicants_file = root / \"applicants.csv\"\n",
    "review_required_file = root / \"applicants_review_required.csv\"\n",
    "reviewed_file = root / \"applicants_reviewed.csv\"\n",
    "final_file = root / \"applicants_final.csv\"\n",
    "winners_file = root / \"winners.csv\"\n",
    "logfile = root / \"log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_awards = 35\n",
    "funding = 3500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify applicants\n",
    "\n",
    "1. Remove any duplicate submissions (same student ID, same DOI)\n",
    "2. Verify that Summit ID has been provided\n",
    "3. Verify eligibility of journals (journal lookup on DOAJ via ISSN/name)\n",
    "4. Mark entries that need manual verification & create CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(applicants_file, na_values=[\"\"])\n",
    "\n",
    "headers = {'Accept': 'application/json'} \n",
    "issn_url = \"https://doaj.org/api/v1/search/journals/issn:{}\"\n",
    "name_url = \"https://doaj.org/api/v1/search/journals/title:{}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Duplicate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Remove duplicate submissions\n",
      "0 duplicate submissions were found among the applications.\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Remove duplicate submissions\")\n",
    "unique_cols = ['Name (First)', 'Name (Last)', 'Email', 'Student ID#', 'Article Title', 'Please provide the ID of your paper in the SFU Research Summit repository']\n",
    "df['duplicate_submission'] = df.duplicated(subset=unique_cols)\n",
    "print(\"{} duplicate submissions were found among the applications.\".format(df['duplicate_submission'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summit IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Checking summit IDs\n",
      "1 submission did not provide a Summit ID\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Checking summit IDs\")\n",
    "df['uploaded_to_summit'] = df['Please provide the ID of your paper in the SFU Research Summit repository'].notna()\n",
    "print(\"{} submission did not provide a Summit ID\".format((~df['uploaded_to_summit']).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Verify eligibility of journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Checking journals\n",
      "Found in DOAJ: 25,requiring attention: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Checking journals\")\n",
    "f = open(str(logfile), \"w\")\n",
    "\n",
    "i = 1\n",
    "application_count = len(df)\n",
    "for ix, row in df[[\"Journal Name\",\"Journal ISSN\"]].iterrows():\n",
    "    name = row[\"Journal Name\"]\n",
    "    issn = row[\"Journal ISSN\"]\n",
    "    \n",
    "    is_open_access = None\n",
    "    \n",
    "    f.write(\"{}/{} - {} ({})\\n\".format(i, application_count, name, issn))\n",
    "    is_issn = re.match(\"[\\S]{4}\\-[\\S]{4}\", issn)\n",
    "    \n",
    "    if is_issn:\n",
    "        r = requests.get(issn_url.format(issn), headers=headers)\n",
    "        total = r.json()['total']\n",
    "        f.write(\"Lookup by ISSN... found {} journals via ISSN\\n\".format(0))\n",
    "        if total == 1:\n",
    "            is_open_access = r.json()['results'][0]['bibjson']['license'][0]['open_access']\n",
    "        elif total < 1:\n",
    "            r = requests.get(name_url.format(name), headers=headers)\n",
    "            total = r.json()['total']\n",
    "            f.write(\"ISSN lookup no results. Lookup by name... found {} journals via name\\n\".format(0))\n",
    "            if total == 1:\n",
    "                is_open_access = r.json()['results'][0]['bibjson']['license'][0]['open_access']\n",
    "    else:\n",
    "        r = requests.get(name_url.format(name), headers=headers)\n",
    "        total = r.json()['total']\n",
    "        f.write(\"ISSN invalid. Lookup by name... found {} journals via name\\n\".format(0))\n",
    "        if total == 1:\n",
    "            is_open_access = r.json()['results'][0]['bibjson']['license'][0]['open_access']\n",
    "    \n",
    "    error_msg = None\n",
    "    if is_open_access is True:\n",
    "        f.write(\"Open Access: Yes\\n\")\n",
    "    elif is_open_access is False:\n",
    "        f.write(\"Open Access: No\\n\")\n",
    "    else:\n",
    "        error_msg = \"Found {} possible candidate journals in DOAJ.\\n\".format(total)\n",
    "        f.write(\"Requiring manual attention: \" + error_msg)\n",
    "        \n",
    "    df.loc[ix, 'found_on_DOAJ'] = is_open_access\n",
    "    df.loc[ix, 'doaj_error'] = error_msg\n",
    "    f.write(\"\\n\")\n",
    "    i = i + 1\n",
    "\n",
    "f.close()\n",
    "print(\"Found in DOAJ: {},requiring attention: {}\".format(df['found_on_DOAJ'].sum(), (df['found_on_DOAJ'] != True).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create file for manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eligible'] = df['found_on_DOAJ'] & df['uploaded_to_summit']\n",
    "df.loc[df['found_on_DOAJ'].isna(), 'eligible'] = \"\"\n",
    "df.to_csv(review_required_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review & double-check the results\n",
    "\n",
    "@todo: write up a guideline for reviewing the results\n",
    "\n",
    "- resources on DOAJ, language around OA\n",
    "\n",
    "add and fill-in remaining values in the `eligible` column in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 winners | 129$ each. Total: 3483$\n"
     ]
    }
   ],
   "source": [
    "eligible = pd.read_csv(reviewed_file, na_values=[\"\"], index_col=0)\n",
    "eligible['winner'] = False\n",
    "winners = eligible[eligible.eligible.astype(bool)]\n",
    "\n",
    "win_ids = []\n",
    "if winners['Student ID#'].nunique() > max_awards:\n",
    "    entries = winners['Student ID#'].value_counts()\n",
    "    while len(win_ids) < max_awards:\n",
    "        total = entries.sum()\n",
    "        weights = [x/total for x in entries]\n",
    "        selected_ids = np.random.choice(entries.keys(), p=weights, size=max_awards-len(win_ids))\n",
    "        selected_ids = list(set(selected_ids))\n",
    "        entries.drop(selected_ids, inplace=True)\n",
    "        win_ids.extend(selected_ids)\n",
    "else:\n",
    "    win_ids = winners['Student ID#'].unique()\n",
    "\n",
    "winners = winners[winners['Student ID#'].isin(win_ids)].drop_duplicates(subset=[\"Student ID#\"])\n",
    "    \n",
    "eligible.loc[winners.index, 'winner'] = True\n",
    "eligible.to_csv(final_file)\n",
    "\n",
    "print(\"{} winners | {}$ each. Total: {}$\".format(len(winners), funding//len(winners), funding//len(winners)*len(winners)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = winners[['Name (First)', 'Name (Last)', 'Email', 'Student ID#',\n",
    "       'Address (Address)', 'Address (Address2)', 'Address (City)',\n",
    "       'Address (State)', 'Address (Zip)', 'Address (Country)', 'If your application is successful, please indicate the method of delivery for your cheque:']]\n",
    "winners = winners.reset_index(drop=True)\n",
    "winners.index = winners.index + 1\n",
    "winners.to_csv(winners_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "- [ ] Create database of applicants and winners to"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altmetrics",
   "language": "python",
   "name": "altmetrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
